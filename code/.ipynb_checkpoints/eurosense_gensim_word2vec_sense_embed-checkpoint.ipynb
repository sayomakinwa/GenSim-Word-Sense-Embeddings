{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, as always!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from corpora import *\n",
    "from my_utils import *\n",
    "import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn2wn_mapping_path = '../resources/bn2wn_mapping.txt'\n",
    "sentences_p = InputSentences('../EuroSense/parsed_low_stp/sentences_precision.txt')\n",
    "sentences_c = InputSentences('../EuroSense/parsed_low_stp/sentences_coverage.txt')\n",
    "sentences_folder = InputSentences('../EuroSense/parsed_low_stp/')\n",
    "test_data_path = \"../wordsim353/combined.tab\"\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_path = '../EuroSense/eurosense.v1.0.high-precision.xml'\n",
    "bn2wn_mapping_path = '../resources/bn2wn_mapping.txt'\n",
    "outfile_path = './EuroSense/parsed_lower/sentences_precision.txt'\n",
    "\n",
    "parse(corpora_path, bn2wn_mapping_path, outfile_path, \"precision\")\n",
    "#1,903,116 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_path = '../EuroSense/EuroSense/eurosense.v1.0.high-coverage.xml'\n",
    "bn2wn_mapping_path = '../resources/bn2wn_mapping.txt'\n",
    "outfile_path = '../EuroSense/parsed_lower/sentenses_coverage.txt'\n",
    "\n",
    "parse(corpora_path, bn2wn_mapping_path, outfile_path, \"coverage\")\n",
    "#1,903,181 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model; Parameter Tuning\n",
    "\n",
    "All tunings are done with the following parameters:\n",
    "\n",
    "dataset = precision; min_count = 5; workers = 5; score for words not found = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tuning 'window'\n",
    "size = 400 (from the reference paper); alpha = 0.025; iter = 5 (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = [3, 5, 6, 8, 10]\n",
    "correlations = []\n",
    "pvals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for windw in windows:\n",
    "    embeddings_path = \"../resources/embeddings_win{}.vec\".format(windw)\n",
    "    \n",
    "    model_win = Word2Vec(sentences_p, size=400, workers=5, window=windw)\n",
    "    save_embeddings(embeddings_path, model_win.wv)\n",
    "    \n",
    "    corr, p, data = score.spearman(test_data_path, embeddings_path, bn2wn_mapping_path)\n",
    "    correlations.append(corr)\n",
    "    pvals.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_win\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8.5, 3)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(windows, correlations, 'b')\n",
    "plt.xlabel('Window')\n",
    "plt.ylabel('Correlation')\n",
    "plt.title(\"Correlation graph\")\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(windows, pvals, 'r')\n",
    "plt.xlabel('Window')\n",
    "plt.ylabel('p-value')\n",
    "plt.title(\"p-value graph\")\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(correlations)\n",
    "print(pvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. High precision corpus vs High coverage corpus vs Combined (not uniquely)\n",
    "window = 6; size = 400 (from the reference paper); alpha = 0.025; iter = 5 (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Precision\", \"Coverage\", \"Combined\"]\n",
    "correlations = []\n",
    "pvals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = \"../resources/embeddings_precision.vec\"\n",
    "model_precision = Word2Vec(sentences_p, size=400, workers=5, window=6)\n",
    "save_embeddings(embeddings_path, model_precision.wv)\n",
    "\n",
    "corr, p, data = score.spearman(test_data_path, embeddings_path, bn2wn_mapping_path)\n",
    "correlations.append(corr)\n",
    "pvals.append(p)\n",
    "#=========================================\n",
    "embeddings_path = \"../resources/embeddings_coverage.vec\"\n",
    "model_coverage = Word2Vec(sentences_c, size=400, workers=5, window=6)\n",
    "save_embeddings(embeddings_path, model_coverage.wv)\n",
    "\n",
    "corr, p, data = score.spearman(test_data_path, embeddings_path, bn2wn_mapping_path)\n",
    "correlations.append(corr)\n",
    "pvals.append(p)\n",
    "#=========================================\n",
    "embeddings_path = \"../resources/embeddings_combined.vec\"\n",
    "model_comb = Word2Vec(sentences_folder, size=400, workers=5, window=6)\n",
    "save_embeddings(embeddings_path, model_comb.wv)\n",
    "\n",
    "corr, p, data = score.spearman(test_data_path, embeddings_path, bn2wn_mapping_path)\n",
    "correlations.append(corr)\n",
    "pvals.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocab size for high precision data is: {:,d}\".format(len(model_precision.wv.vocab)))\n",
    "print(\"Vocab size for high coverage data is: {:,d}\".format(len(model_coverage.wv.vocab)))\n",
    "print(\"Vocab size for the combined data is: {:,d}\".format(len(model_comb.wv.vocab)))\n",
    "\n",
    "del model_precision\n",
    "del model_coverage\n",
    "del model_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_width = 0.3\n",
    "pos1 = np.arange(len(correlations))\n",
    "pos2 = [x + bar_width for x in pos1]\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8, 3)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(pos1, correlations, color='blue', width=bar_width)\n",
    "plt.title(\"Correlation graph\")\n",
    "plt.xticks([y for y in range(len(correlations))], names)\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(pos2, _pvals, color='red', width=bar_width)\n",
    "plt.title(\"p-value graph\")\n",
    "plt.xticks([y + bar_width for y in range(len(correlations))], names)\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(correlations)\n",
    "print(pvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tuning vector 'size'\n",
    "window = 6; alpha = 0.025; iter = 5 (default); combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [100, 200, 300, 400, 500, 600]\n",
    "correlations = []\n",
    "pvals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sizes:\n",
    "    embeddings_path = \"../resources/embeddings_size{}.vec\".format(s)\n",
    "    \n",
    "    model_size = Word2Vec(sentences_folder, workers=5, size=s, window=6)\n",
    "    save_embeddings(embeddings_path, model_size.wv)\n",
    "    \n",
    "    corr, p, data = score.spearman(test_data_path, embeddings_path, bn2wn_mapping_path)\n",
    "    correlations.append(corr)\n",
    "    pvals.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_size\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8.5, 3)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(sizes, correlations, 'b')\n",
    "plt.xlabel('Vector sizes')\n",
    "plt.ylabel('Correlation')\n",
    "plt.title(\"Correlation graph for vector 'size'\")\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(sizes, pvals, 'r')\n",
    "plt.xlabel('Vector sizes')\n",
    "plt.ylabel('p-value')\n",
    "plt.title(\"p-value graph for vector 'size'\")\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(correlations)\n",
    "print(pvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Tuning starting 'alpha'\n",
    "size = 500; window = 6; iter = 5 (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.02, 0.04, 0.06, 0.08, 0.1]\n",
    "correlations = []\n",
    "pvals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alph in alphas:\n",
    "    embeddings_path = \"../resources/embeddings_alpha{}.vec\".format(alph)\n",
    "    \n",
    "    model_alph = Word2Vec(sentences_folder, workers=5, size=500, window=6, alpha=alph)\n",
    "    save_embeddings(embeddings_path, model_alph.wv)\n",
    "    \n",
    "    corr, p, data = score.spearman(test_data_path, embeddings_path, bn2wn_mapping_path)\n",
    "    correlations.append(corr)\n",
    "    pvals.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.02, 0.04, 0.06, 0.08, 0.1]\n",
    "_correlations = []\n",
    "_pvals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_alph\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8.5, 3)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(alphas, correlations, 'b')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Correlation')\n",
    "plt.title(\"Correlation graph for alpha\")\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(alphas, pvals, 'r')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('p-value')\n",
    "plt.title(\"p-value graph for alpha\")\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(correlations)\n",
    "print(pvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Tuning starting 'alpha'\n",
    "size = 500; window = 6; iter = 5 (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.005, 0.007, 0.009, 0.01, 0.02]\n",
    "correlations = []\n",
    "pvals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alph in alphas:\n",
    "    embeddings_path = \"../resources/embeddings_alpha{}.vec\".format(alph)\n",
    "    \n",
    "    model_alph = Word2Vec(sentences_folder, workers=5, size=500, window=6, alpha=alph)\n",
    "    save_embeddings(embeddings_path, model_alph.wv)\n",
    "    \n",
    "    corr, p, data = score.spearman(test_data_path, embeddings_path, bn2wn_mapping_path)\n",
    "    correlations.append(corr)\n",
    "    pvals.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model_alph\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8.5, 3)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(alphas, correlations, 'b')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Correlation')\n",
    "plt.title(\"Correlation graph for alpha\")\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(alphas, pvals, 'r')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('p-value')\n",
    "plt.title(\"p-value graph for alpha\")\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(correlations)\n",
    "print(pvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Tuning epochs 'iter'\n",
    "alpha = 0.009; size = 500; window = 6;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [30]\n",
    "correlations = []\n",
    "pvals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in epochs:\n",
    "    embeddings_path = \"../resources/embeddings_epochs{}.vec\".format(epoch)\n",
    "    \n",
    "    model_iter = Word2Vec(sentences_folder, workers=5, size=500, window=6, iter=epoch)\n",
    "    save_embeddings(embeddings_path, model_iter.wv)\n",
    "    \n",
    "    corr, p, data = score.spearman(test_data_path, embeddings_path, bn2wn_mapping_path)\n",
    "    correlations.append(corr)\n",
    "    pvals.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs = [40, 50]\n",
    "\n",
    "epochs = [30]\n",
    "# correlations = []\n",
    "# pvals = []\n",
    "\n",
    "model_iter = Word2Vec(sentences_folder, workers=5, size=500, window=6, alpha=0.03, iter=30)\n",
    "save_embeddings(embeddings_path, model_iter.wv)\n",
    "\n",
    "corr, p, data = score.spearman(test_data_path, embeddings_path, bn2wn_mapping_path)\n",
    "correlations.append(corr)\n",
    "pvals.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings_path)\n",
    "for epoch in epochs:\n",
    "    embeddings_path = \"../resources/embeddings_epochs{}.vec\".format(epoch)\n",
    "    \n",
    "    model_iter = Word2Vec(sentences_folder, workers=5, size=500, window=6, iter=epoch)\n",
    "    save_embeddings(embeddings_path, model_iter.wv)\n",
    "    \n",
    "    corr, p, data = score.spearman(test_data_path, embeddings_path, bn2wn_mapping_path)\n",
    "    correlations.append(corr)\n",
    "    pvals.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model_iter\n",
    "\n",
    "# plt.plot(alphas, correlations, 'b', alphas, pvals, 'r')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.text(45, .135, 'Correlation')\n",
    "# plt.text(48, .008, 'p-value')\n",
    "# plt.show()\n",
    "\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = []\n",
    "pvals = []\n",
    "\n",
    "# embeddings_path = \"../resources/embeddings_size{}_lr{}_epo{}.vec\".format(100, 0.045, 10)\n",
    "\n",
    "# model_iter = Word2Vec(sentences_folder, workers=5, size=100, window=6, alpha=0.045, iter=10)\n",
    "# save_embeddings(embeddings_path, model_iter.wv)\n",
    "\n",
    "# corr, p, data = score.spearman(test_data_path, embeddings_path, bn2wn_mapping_path)\n",
    "# correlations.append(corr)\n",
    "# pvals.append(p)\n",
    "\n",
    "\n",
    "embeddings_path = \"../resources/embeddings_prec_size{}_win{}_neg{}_lr{}_epo{}.vec\".format(100, 6, 20, 0.09, 10)\n",
    "\n",
    "model_iter = Word2Vec(sentences_p, workers=5, size=100, window=6, alpha=0.09, negative=20, iter=10)\n",
    "save_embeddings(embeddings_path, model_iter.wv)\n",
    "\n",
    "corr, p, data = score.spearman(test_data_path, embeddings_path)\n",
    "correlations.append(corr)\n",
    "pvals.append(p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model_iter.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senses_only_model = KeyedVectors.load_word2vec_format(embeddings_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(senses_only_model.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#embeddings_path = \"../resources/embeddings_size100_win6_neg20_lr0.09_epo10.vec\"\n",
    "embeddings_path = \"../resources/embeddings_size100_win6_neg5_lr0.09_epo10.vec\"\n",
    "model = KeyedVectors.load_word2vec_format(embeddings_path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"implement_bn:00082712v\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "keys = ['believe_bn:00083369v', 'agree_bn:00082476v', 'field_bn:00007985n', 'power_bn:00063940n', 'implement_bn:00082712v']\n",
    "\n",
    "embedding_clusters = []\n",
    "word_clusters = []\n",
    "for word in keys:\n",
    "    embeddings = []\n",
    "    words = []\n",
    "    for similar_word, _ in model.most_similar(word, topn=20):\n",
    "        words.append(similar_word)\n",
    "        embeddings.append(model[similar_word])\n",
    "    embedding_clusters.append(embeddings)\n",
    "    word_clusters.append(words)\n",
    "    \n",
    "embedding_clusters = np.array(embedding_clusters)\n",
    "n, m, k = embedding_clusters.shape\n",
    "tsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_similar_words_plot('Similar words to some randomly selected sense embeddings', keys, embeddings_en_2d, word_clusters, 0.7, 'similar_words.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "keys = [\"love\", \"sex\", \"smart\", \"student\", \"plane\", \"car\"]\n",
    "\n",
    "\n",
    "embedding_clusters = []\n",
    "word_clusters = []\n",
    "for word in keys:\n",
    "    senses, embeddings = get_first_similar_words(word, model)\n",
    "    embedding_clusters.append(embeddings)\n",
    "    word_clusters.append(senses)\n",
    "    \n",
    "embedding_clusters = np.array(embedding_clusters)\n",
    "n, m, k = embedding_clusters.shape\n",
    "tsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_similar_words_plot('Senses of some word taken from the combined.tab file', keys, embeddings_en_2d, word_clusters, 0.7, 'similar_words2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
